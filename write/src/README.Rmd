---
title: Network Scale Up for estimating the size of hard-to-reach subpopulations
author:
- "[Tarak Shah](https://tarakc02.github.io/)"
output:
    github_document:
        toc: true
        html_preview: true
---

## Background

I learned about [the network scale up
method](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4777323/) at the 2020 KDD
humanitarian mapping workshop, in a presentation on the paper [CoronaSurveys:
Using Surveys with Indirect Reporting to Estimate the Incidence and Evolution
of Epidemics](https://arxiv.org/abs/2005.12783).

The idea is that to estimate the size of a hard-to-count population, you
randomly survey a handful of people and ask them:

a) how many people do you know in all (the person's *degree*), and

b) how many people do you know in the subpopulation of interest? We use the
results to estimate the proportion of the overall population who are in the
subpopulation of interest.

Unfortunately, it turns out most people can't answer \(a\) very well. But, we
can estimate how many people someone knows by asking them a series of questions
about members of subpopulations of known size ("how many people do you know
named Michael?", "how many people do you know with a graduate degree?", etc.).

So putting things together, our method is to ask randomly selected people a
battery of questions, some of them about known-size subpopulations, and some
about subpopulations whose size we want to estimate.  Then we jointly estimate
each respondent's degree (the number of people they know) as well as the sizes
of the unknown subpopulations. The linked paper starts with a simple model and
builds it up by first allowing partial pooling of degree estimates, then
allowing the likelihood of knowing someone in a given subpopulation to vary by
individual and subpopulation, then adding further parameters for various types
of reporting bias (e.g. transmission error, where someone doesn't know that an
acquaintance belongs to a given group, or recall bias, where someone forgets to
count people when asked to quickly report on their contacts).

## Simulation

To get a better understanding of the method, I coded up [some
simulations](https://github.com/tarakc02/network-scale-up). I simulated from
two different models, the **random degree** model, which allows each
respondent's degree (the total number of people they know) to vary, and the
**barrier effects** model, which also allows the probability of knowing someone
in the subpopulations of interest to vary. From each model, I simulated a
survey of 25 people, and one of 100 people, so 4 simulated datasets in all.
Below is the distribution of degree for the simulated respondents in each
simulation:

```{r, message = FALSE, warn = FALSE, echo = FALSE}
library(purrr)
library(yaml)
library(stringr)
library(dplyr)
library(tidyr)

logfiles <- list.files(here::here("simulate/output"),
                       pattern = "*.yaml", full.names = TRUE)

name_from_fn <- function(fns)
    str_match(fns, "output/([^.]+)\\.rds\\-log\\.yaml")[,2]

sim_stats <- set_names(logfiles, name_from_fn) %>%
    map(read_yaml)

map(sim_stats, "degrees") %>%
    map(quantile, c(0, .1, .3, .5, .7, .9, 1)) %>%
    map(~tibble(quantile = names(.), x = .)) %>%
    bind_rows(.id = "model") %>%
    pivot_wider(names_from = quantile, values_from = x)
```

And summaries of other aspects of the simulations, including the (unobserved)
subpopulation sizes, which will be our main target for inference:

```{r, echo = FALSE}
summarise_sim <- function(sim, sim_name) {
    cat("\n########\n")
    cat("sim_name                      : ", sim_name, "\n")
    cat("population                    : ", sim$population, "\n",
        "respondents                   : ", sim$n_respondents, "\n",
        "size of unknown subpopulations: ", paste(sim$unknown_populations,
                                                  collapse = "  "), "\n",
        sep = "")
    return(sim)
}

iwalk(sim_stats, summarise_sim)
```

## Models

Find the stan models used to fit the random degree and barrier effects models
in the [`fit-model`](../fit-model) directory.

## Assessing fit without access to ground truth

The data, in these examples, looks like a vector  of integers, responses to the
question "how many people do you know in group X?" The main goal for doing
inference is estimating the size of the subpopulation of interest. The
probability model connects that value to the observed data (the vector of
responses). In real life, I wouldn't have access to the true size of the
subpopulation to compare model predictions to (if I did, I wouldn't have to
make a model or collect data in the first place), but I can use the fitted
model to simulate new data sets. If things have fit well, then the actual data
set should look like the simulated data sets. So, for instance, the mean in the
actual data should fall within the range of means seen in the simulated
datasets.

In most situations, we'll have a model that fits some aspects of the data, and
not others, and deciding whether a model is useful will depend on whether the
comparisons (between data and model posterior) that we make are relevant to our
ultimate questions of interest. Since I'm ultimately interested in estimating
the sizes of the unknown subpopulations, I decided to focus on the vectors of
responses to the three questions about the unknown subpopulations. For specific
statistical tests, I chose three summary statistics: the mean, the standard
deviation, and the 95th percentile. I calculate all three summary statistics on
the observed data, and on simulated replicates of the data, and then I see
where the summary of the observed data fits in the distribution of summaries
over the replicate datasets.

See the [`assess-fit/output`](../assess-fit/output) directory for all of the
resulting comparisons. Below are a couple of representative examples. First, we
see how each model looked on the random degree simulated data with 100
respondents:

|![](../assess-fit/output/checkfit-rd-100-rd.png?raw=true) |![](../assess-fit/output/checkfit-rd-100-bfx.png?raw=true)|
|-|-|

Both models fit the model well, at least according to this test.

Now the two models on the barrier effects simulated data with 100 respondents:

|![](../assess-fit/output/checkfit-bfx-100-rd.png?raw=true) |![](../assess-fit/output/checkfit-bfx-100-bfx.png?raw=true)|
|-|-|

Since the random degree model doesn't account for variation in exposure to the
subpopulation, the fitted model cannot account for the amount of variation, or
the more extreme values that arise due to that variation.

## Comparing to ground truth

Since in this example I am modeling simulated data, I can compare inferred
values from my model to the ground truth values. The following table
summarizes, for each subpopulation within each dataset and for each model, the
point estimate along with associated 95% credible interval, alongside the
ground truth:

```{r, message = FALSE, warn = FALSE, echo = FALSE}
library(rstan)
library(tidybayes)
library(ggplot2)
library(stringr)

bn <- function(fn) tools::file_path_sans_ext(basename(fn))

fns <- list.files(here::here("fit-model/output"),
           full.names = TRUE, pattern = "*.rds$") %>%
    set_names(bn)

fits <- map(fns, readRDS)

suppressWarnings(
 posteriors <- map_dfr(fits,
                       gather_draws,
                       subpop_size[subpop],
                       ndraws = 1000,
                       .id = "model_data") %>%
 ungroup %>%
 mutate(data = str_match(model_data, "^fit-([^\\-]+-[0-9]+)")[,2],
        model = str_extract(model_data, "[^\\-]+$"))
)

true_subpops <- map(sim_stats, "unknown_populations")

true_subpop_sizes <- map(sim_stats, "unknown_populations") %>%
    map_dfr(~tibble(subpop = seq_along(.x), truth = .x), .id = "data")

posteriors %>%
    filter(data %in% c("bfx-100", "rd-100")) %>%
    group_by(model, data, subpop) %>%
    summarise(lo = quantile(.value, .025),
              mid = median(.value),
              hi = quantile(.value, .975),
              ecdf = list(ecdf(.value)),
              .groups = "drop") %>%
    inner_join(true_subpop_sizes, by = c("data", "subpop")) %>%
    mutate(in_interval = truth >= lo & truth <= hi) %>%
    mutate(across(c(lo, mid, hi),
                  ~str_trim(format(round(.),
                                   big.mark = ","))),
           truth_pvalue = map2_dbl(ecdf, truth, ~.x(.y))) %>%
    transmute(model, data, subpop,
              truth = format(round(truth), big.mark = ","),
              "estimate and 95% interval" = str_glue("{mid} ({lo} - {hi})"),
              in_interval, "p-value (truth)" = truth_pvalue) %>%
    arrange(desc(model), desc(data)) %>%
    knitr::kable()
```

The `rd` model fails to recover the unknown population sizes when the true data
generating process allows for individual variation in likelihood of knowing
someone in the unknown subpopulation.

To get a sense of how the number of respondents affects the uncertainty in our
estimates, here's the same summary on the datasets with 25, instead of 100,
respondents:

```{r}
posteriors %>%
    filter(data %in% c("bfx-25", "rd-25")) %>%
    group_by(model, data, subpop) %>%
    summarise(lo = quantile(.value, .025),
              mid = median(.value),
              hi = quantile(.value, .975),
              ecdf = list(ecdf(.value)),
              .groups = "drop") %>%
    inner_join(true_subpop_sizes, by = c("data", "subpop")) %>%
    mutate(in_interval = truth >= lo & truth <= hi) %>%
    mutate(across(c(lo, mid, hi),
                  ~str_trim(format(round(.),
                                   big.mark = ","))),
           truth_pvalue = map2_dbl(ecdf, truth, ~.x(.y))) %>%
    transmute(model, data, subpop,
              truth = format(round(truth), big.mark = ","),
              "estimate and 95% interval" = str_glue("{mid} ({lo} - {hi})"),
              in_interval, "p-value (truth)" = truth_pvalue) %>%
    arrange(desc(model), desc(data)) %>%
    knitr::kable()
```

